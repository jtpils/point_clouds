{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\">Imports</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\">Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quaternion-functions:\" data-toc-modified-id=\"Quaternion-functions:-2.1\">Quaternion functions:</a></span></li><li><span><a href=\"#General-purpose-ops-(Utils)\" data-toc-modified-id=\"General-purpose-ops-(Utils)-2.2\">General purpose ops (Utils)</a></span></li></ul></li><li><span><a href=\"#Projection-module\" data-toc-modified-id=\"Projection-module-3\">Projection module</a></span><ul class=\"toc-item\"><li><span><a href=\"#Camera-transformation\" data-toc-modified-id=\"Camera-transformation-3.1\">Camera transformation</a></span></li><li><span><a href=\"#Voxels-from-point-clouds\" data-toc-modified-id=\"Voxels-from-point-clouds-3.2\">Voxels from point clouds</a></span></li><li><span><a href=\"#Voxels-smoothing\" data-toc-modified-id=\"Voxels-smoothing-3.3\">Voxels smoothing</a></span></li><li><span><a href=\"#Projection-and-point-cloud-dropout\" data-toc-modified-id=\"Projection-and-point-cloud-dropout-3.4\">Projection and point cloud dropout</a></span></li></ul></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-4\">Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Common\" data-toc-modified-id=\"Common-4.1\">Common</a></span></li><li><span><a href=\"#Point-cloud-decoder\" data-toc-modified-id=\"Point-cloud-decoder-4.2\">Point cloud decoder</a></span></li><li><span><a href=\"#Pose-decoder\" data-toc-modified-id=\"Pose-decoder-4.3\">Pose decoder</a></span></li><li><span><a href=\"#SupervisedModel\" data-toc-modified-id=\"SupervisedModel-4.4\">SupervisedModel</a></span></li><li><span><a href=\"#Unsupervised-model\" data-toc-modified-id=\"Unsupervised-model-4.5\">Unsupervised model</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-5\">Data</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-6\">Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overfit-one-batch\" data-toc-modified-id=\"Overfit-one-batch-6.1\">Overfit one batch</a></span></li><li><span><a href=\"#Training-loop\" data-toc-modified-id=\"Training-loop-6.2\">Training loop</a></span></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-6.3\">Train models</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-7\">Evaluation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:30.563026Z",
     "start_time": "2019-12-18T11:45:30.555576Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:31.648143Z",
     "start_time": "2019-12-18T11:45:30.902065Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:04:47.184746Z",
     "start_time": "2019-12-18T12:04:47.175716Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as T\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "General tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quaternion functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:38.469011Z",
     "start_time": "2019-12-18T11:45:38.445122Z"
    }
   },
   "outputs": [],
   "source": [
    "def points2quat(v):\n",
    "    \"Convert xyz points to quaternions\"\n",
    "    assert len(v.shape) == 3\n",
    "    assert v.size(-1) == 3\n",
    "    return F.pad(v, (1, 0, 0, 0))\n",
    "\n",
    "def quatmul(q1, q2):\n",
    "    \"Multiply quaternions\"\n",
    "    w1, x1, y1, z1 = torch.unbind(q1, dim=-1)\n",
    "    w2, x2, y2, z2 = torch.unbind(q2, dim=-1)\n",
    "\n",
    "    w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2\n",
    "    x = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2\n",
    "    y = w1 * y2 + y1 * w2 + z1 * x2 - x1 * z2\n",
    "    z = w1 * z2 + z1 * w2 + x1 * y2 - y1 * x2\n",
    "    return torch.stack([w, x, y, z], dim=-1)\n",
    "\n",
    "def quatconj(q):\n",
    "    \"Conjugate of quaternion\"\n",
    "    m = q.new(4).fill_(-1)\n",
    "    m[0] = 1.0\n",
    "    return q * m \n",
    "\n",
    "def quatrot(v, q, inverse=False):\n",
    "    \"Rotate points v [b, n, 3] with quaternions q [b, 4]\"\n",
    "    q = F.normalize(q, dim=-1)\n",
    "    q = q[:, None, :]\n",
    "    q_ = quatconj(q)\n",
    "    v = points2quat(v)\n",
    "\n",
    "    if inverse:\n",
    "        wxyz = quatmul(quatmul(q_, v), q)\n",
    "    else:\n",
    "        wxyz = quatmul(quatmul(q, v), q_)\n",
    "        \n",
    "    if len(wxyz.shape) == 2:\n",
    "        wxyz = wxyz.unsqueeze(0)\n",
    "\n",
    "    return wxyz[:, :, 1:4]\n",
    "\n",
    "\n",
    "def quat_from_campos(pos):\n",
    "    \"Convert blender camera format `pos` to torch tensor quaternion [w, x, y, z]\"\n",
    "    cx, cy, cz = pos[0]\n",
    "    camDist = math.sqrt(cx * cx + cy * cy + cz * cz)\n",
    "    cx = cx / camDist\n",
    "    cy = cy / camDist\n",
    "    cz = cz / camDist\n",
    "    t = math.sqrt(cx * cx + cy * cy)\n",
    "    tx = cx / t\n",
    "    ty = cy / t\n",
    "    yaw = math.acos(tx)\n",
    "    if ty > 0:\n",
    "        yaw = 2 * math.pi - yaw\n",
    "\n",
    "    roll = 0\n",
    "    pitch = math.asin(cz)\n",
    "    yaw = yaw + math.pi\n",
    "\n",
    "    quat = Rotation.from_euler(\"yzx\", [yaw, pitch, roll]).as_quat()\n",
    "    quat = np.r_[quat[-1], quat[:-1]]\n",
    "\n",
    "    return torch.tensor(quat.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General purpose ops (Utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:38.751978Z",
     "start_time": "2019-12-18T11:45:38.743696Z"
    }
   },
   "outputs": [],
   "source": [
    "def repeat_tensor_batch(tensor, times):\n",
    "    \"Repeat tensor `times` times for each element in batch\"\n",
    "    if tensor is None: return\n",
    "\n",
    "    data_shape = tensor.shape[1:]\n",
    "    repeats = [1, times] + [1] * len(data_shape)\n",
    "\n",
    "    expanded = tensor.unsqueeze(1).repeat(*repeats)\n",
    "    return expanded.view(-1, *data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:41:16.340977Z",
     "start_time": "2019-12-18T12:41:16.328348Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_projections_img(model, imgs, poses, masks):\n",
    "    \"Generate grid with model projections, gt projections and input images\"\n",
    "    device = next(model.parameters()).device\n",
    "    proj, *_ = model(imgs[0].unsqueeze(0).to(device), poses.to(device))\n",
    "    proj = proj.detach().cpu()\n",
    "\n",
    "    grid = torch.cat([\n",
    "        F.interpolate(imgs, scale_factor=1/2, mode='bilinear', align_corners=True),\n",
    "        F.interpolate(masks.unsqueeze(1), scale_factor=1/2, mode='bilinear', align_corners=True).repeat(1, 3, 1, 1),\n",
    "        proj.unsqueeze(1).repeat(1, 3, 1, 1),\n",
    "    ])\n",
    "    \n",
    "    grid = make_grid(grid, nrow=imgs.size(0))\n",
    "    return F.interpolate(grid.unsqueeze(0), scale_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection module\n",
    "Most of the module is implemented as functions, as almost all operations don't have learnable parameters. I tried to set default hyperparameter values as mentioned in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera transformation \n",
    "camera had same $z$ coordinate with different angles and $x$, $y$ positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:39.676553Z",
     "start_time": "2019-12-18T11:45:39.668334Z"
    }
   },
   "outputs": [],
   "source": [
    "def pc_camera_transform(pc, rotation, focal_lenght=1.875, camera_distance=2.0):\n",
    "    \"Transform pontcloud `pc` to camera coordinates with `rotation`\"\n",
    "    \n",
    "    pc = quatrot(pc, rotation)\n",
    "    zs, ys, xs = torch.unbind(pc, dim=2)\n",
    "    \n",
    "    xs = xs * focal_lenght / (zs + camera_distance)\n",
    "    ys = ys * focal_lenght / (zs + camera_distance)\n",
    "\n",
    "    return torch.stack([zs, ys, xs], dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxels from point clouds\n",
    "\n",
    "Fit point cloud to 3d grid with trilinear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:40.259273Z",
     "start_time": "2019-12-18T11:45:40.242203Z"
    }
   },
   "outputs": [],
   "source": [
    "def pc_voxels(pc, size=64, eps=1e-6):\n",
    "    \"Create voxels of `[size]*3` from pointcloud `pc`\"\n",
    "    # save for later\n",
    "    vox_size = pc.new(3).fill_(size)\n",
    "    bs = pc.size(0)\n",
    "    n = pc.size(1)\n",
    "    \n",
    "    # check borders\n",
    "    valid = ((pc < 0.5 - eps) & (pc > -0.5 + eps)).all(dim=-1).view(-1)\n",
    "    grid = (pc + 0.5) * (vox_size - 1)\n",
    "    grid_floor = grid.floor()\n",
    "\n",
    "    grid_idxs = grid_floor.long()\n",
    "    batch_idxs = torch.arange(bs)[:, None, None].repeat(1, n, 1).to(pc.device)\n",
    "    # idxs of form [batch, z, y, x] where z, y, x discretized indecies in voxel\n",
    "    idxs = torch.cat([batch_idxs, grid_idxs], dim=-1).view(-1, 4)\n",
    "    idxs = idxs[valid]\n",
    "\n",
    "    # trilinear interpolation\n",
    "    r = grid - grid_floor\n",
    "    rr = [1. - r, r]\n",
    "    voxels = []\n",
    "    voxels_t = pc.new(bs, size, size, size).fill_(0)\n",
    "\n",
    "    def trilinear_interp(pos):\n",
    "        update = rr[pos[0]][..., 0] * rr[pos[1]][..., 1] * rr[pos[2]][..., 2]\n",
    "        update = update.view(-1)[valid]\n",
    "        \n",
    "        shift_idxs = torch.LongTensor([[0] + pos]).to(pc.device)\n",
    "        shift_idxs = shift_idxs.repeat(idxs.size(0), 1)\n",
    "        update_idxs = idxs + shift_idxs\n",
    "        valid_shift = update_idxs < size\n",
    "        voxels_t.index_put_(torch.unbind(update_idxs, dim=1), update, accumulate=True)\n",
    "\n",
    "        return voxels_t\n",
    "        \n",
    "    \n",
    "    for k in range(2):\n",
    "        for j in range(2):\n",
    "            for i in range(2):\n",
    "                voxels.append(trilinear_interp([k, j, i]))\n",
    "    \n",
    "    return torch.stack(voxels).sum(dim=0).clamp(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxels smoothing\n",
    "\n",
    "Fast gaussian smoothing of a voxel. There was an issue with speed of pytorch conv3d backward pass in this operation. I solved it by reordering batch and channels dimensions and using groups. It turned out to be 10x faster. Batch parallelization of `conv3d` vs channel parallelization (second is faster in this case with $1$ channel in voxel and $\\approx 40$ voxels per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:40.687961Z",
     "start_time": "2019-12-18T11:45:40.674113Z"
    }
   },
   "outputs": [],
   "source": [
    "def smoothing_kernel(sigma, kernel_size=21):\n",
    "    \"Generate 3 separate gaussian kernels with `sigma` stddev\"\n",
    "    x = torch.arange(-kernel_size//2 + 1., kernel_size//2 + 1., device=sigma.device)\n",
    "    kernel_1d = torch.exp(-x**2 / (2. * sigma**2))\n",
    "    kernel_1d = kernel_1d / kernel_1d.sum()\n",
    "\n",
    "    k1 = kernel_1d.view(1, 1, 1, 1, -1)\n",
    "    k2 = kernel_1d.view(1, 1, 1, -1, 1)\n",
    "    k3 = kernel_1d.view(1, 1, -1, 1, 1)\n",
    "    return [k1, k2, k3]\n",
    "\n",
    "def voxels_smooth(voxels, kernels, scale=None):\n",
    "    \"Apply gaussian blur to voxels with separable `kernels` then `scale`\"\n",
    "    assert isinstance(kernels, list)\n",
    "\n",
    "    # add fake channel for convs\n",
    "    bs = voxels.size(0)\n",
    "    voxels = voxels.unsqueeze(0)\n",
    "\n",
    "    for k in kernels:\n",
    "        # add padding for kernel dimension\n",
    "        padding = [0] * 3\n",
    "        padding[np.argmax(k.shape) - 2] = max(k.shape) // 2\n",
    "\n",
    "        voxels = F.conv3d(voxels, k.repeat(bs, 1, 1, 1, 1), stride=1, padding=padding, groups=bs)\n",
    "\n",
    "    voxels = voxels.squeeze(0)\n",
    "\n",
    "    if scale is not None:\n",
    "        voxels = voxels * scale.view(-1, 1, 1, 1)\n",
    "        voxels = voxels.clamp(0, 1)\n",
    "\n",
    "    return voxels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection and point cloud dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:41.104252Z",
     "start_time": "2019-12-18T11:45:41.093470Z"
    }
   },
   "outputs": [],
   "source": [
    "def drc_prob(voxels, clip_val=1e-5):\n",
    "    \"Compute termination probabilities from part 4 https://arxiv.org/pdf/1810.09381.pdf\"\n",
    "    inp = voxels.permute(1, 0, 2, 3)\n",
    "    inp = inp.clamp(clip_val, 1.0 - clip_val)\n",
    "    zero = voxels.new(1, inp.size(1), inp.size(2), inp.size(3)).fill_(clip_val)\n",
    "\n",
    "    y = torch.log(inp)\n",
    "    x = torch.log(1 - inp)\n",
    "\n",
    "    r = torch.cumsum(x, dim=0)\n",
    "    p1 = torch.cat([zero, r], dim=0)\n",
    "    p2 = torch.cat([y, zero], dim=0)\n",
    "\n",
    "    p = p1 + p2\n",
    "    return torch.exp(p).permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class, which handles projection from point cloud(s) to images of this ponit cloud(s) at different rotatoins. The result is $N_{\\mathrm{batch}} \\cdot N_{\\mathrm{views}} \\times N_{\\mathrm{img}} \\times N_{\\mathrm{img}}$, so each point cloud is rotated multiple times (read: repeated for each view in batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:41.442049Z",
     "start_time": "2019-12-18T11:45:41.433034Z"
    }
   },
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    \"Diffefentiable point cloud projection module\"\n",
    "    def __init__(self, vox_size=64, smooth_ks=21, smooth_sigma=3.0):\n",
    "        super().__init__()\n",
    "        self.vox_size = vox_size\n",
    "        self.ks = smooth_ks\n",
    "        self.register_buffer('sigma', torch.tensor(smooth_sigma))\n",
    "\n",
    "    def forward(self, pc, rotation, scale=None):\n",
    "        \"Project points `pc` to camera givne by `transform`\"\n",
    "        pc = pc_camera_transform(pc, rotation)\n",
    "        voxels = pc_voxels(pc, self.vox_size)\n",
    "        smooth = voxels_smooth(voxels, kernels=smoothing_kernel(self.sigma, self.ks), scale=scale)\n",
    "\n",
    "        prob = drc_prob(smooth)\n",
    "        proj = prob[:, :-1].sum(1).flip(1)\n",
    "        return proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point cloud dropout keeps only `keep_prob` points in each point cloud of a batch (different points for each example in one batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:41.802634Z",
     "start_time": "2019-12-18T11:45:41.792696Z"
    }
   },
   "outputs": [],
   "source": [
    "class PointCloudDropout(nn.Module):\n",
    "    \"Drop random portions of pointclouds `pc`\"\n",
    "    def __init__(self, keep_prob=0.07):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "    def forward(self, pc):\n",
    "        bs, n_points = pc.size(0), pc.size(1)\n",
    "        n_keep = math.ceil(n_points * self.keep_prob)\n",
    "\n",
    "        batch_idxs = repeat_tensor_batch(torch.arange(bs), n_keep)\n",
    "        point_idxs = torch.cat([torch.randperm(n_points)[:n_keep] for i in range(bs)])\n",
    "\n",
    "        return pc[batch_idxs, point_idxs].view(bs, n_keep, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Both camera supervised and unsupervised models are implemented below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common \n",
    "\n",
    "blocks used in modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:42.615219Z",
     "start_time": "2019-12-18T11:45:42.605741Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(in_ch, out_ch, ks=3, stride=1, padding=1, bn=False, act=True):\n",
    "    \"Basic convolutional block with relu and batchnorm\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, ks, stride, padding, bias=not bn),\n",
    "        nn.ReLU(True) if act else nn.Identity(),\n",
    "        nn.BatchNorm2d(out_ch) if bn else nn.Identity(),\n",
    "    )\n",
    "\n",
    "def pose_predictor(n_ft):\n",
    "    \"Pose predictor with 2 hidden layers\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(n_ft, n_ft),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(n_ft, n_ft),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(n_ft, 4),\n",
    "    )\n",
    "\n",
    "def weight_init(m):\n",
    "    \"Kaiming normal init for relu with slope 0 for linear and conv layers\"\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight.data, a=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder is almost as in original paper, but with batch normalization and ReLU activations with kaiming normal initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T19:08:20.796829Z",
     "start_time": "2019-12-19T19:08:20.783000Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Encodes input images\"\n",
    "    def __init__(self, img_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(                       \n",
    "            conv_block(3, 16,  ks=5, stride=2, padding=2),\n",
    "            conv_block(16, 16, ks=3, stride=2),            \n",
    "            conv_block(16, 16, ks=3, stride=1),          \n",
    "            conv_block(16, 16, ks=3, stride=2),           \n",
    "            conv_block(16, 16, ks=3, stride=1),\n",
    "            conv_block(16, 16, ks=3, stride=2),           \n",
    "            conv_block(16, 16, ks=3, stride=1),\n",
    "            conv_block(16, 16, ks=3, stride=2),          \n",
    "            conv_block(16, 16, ks=3, stride=1),\n",
    "        )\n",
    "        features_size = img_size // 8\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(features_size**2, 1024, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 1024),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        conv_features = self.convs(img)\n",
    "        features = self.features(conv_features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point cloud decoder \n",
    "\n",
    "used to generate point clouds from hidden vector of encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:44.108899Z",
     "start_time": "2019-12-18T11:45:44.101377Z"
    }
   },
   "outputs": [],
   "source": [
    "class PointCloudDecoder(nn.Module):\n",
    "    def __init__(self, num_points, hidden_dim=1024, predict_scale=True):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.predict_scale = predict_scale\n",
    "        self.pc_decoder = nn.Linear(hidden_dim, num_points * 3)\n",
    "        self.scale_decoder = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"Transform hidden vector to pointcloud\"\n",
    "        # predict pointcloud\n",
    "        pc = self.pc_decoder(z)\n",
    "        pc = pc.view(-1, self.num_points, 3)\n",
    "        pc = torch.tanh(pc) / 2.0\n",
    "\n",
    "        scale = None\n",
    "        if self.predict_scale:\n",
    "            scale = self.scale_decoder(z)\n",
    "            scale = torch.sigmoid(scale)\n",
    "\n",
    "        return pc, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pose branch `hidden_dim` is slightly bigger than in paper. Pose decoder in `eval` mode returns one pose for each example in batch. In `train` mode it returns `num_candidates` poses for each example (one additional pose is student prediction). In this case poses are repeated in batch dimension like `num_candidates` poses for first example, ..., `num_candidates` poses for nth example. In this setting we can use `repeat_tensor_batch` to replicate point clouds for each view and get projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:45:45.449065Z",
     "start_time": "2019-12-18T11:45:45.438876Z"
    }
   },
   "outputs": [],
   "source": [
    "class PoseDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_candidates=4):\n",
    "        \"Predict `num_candidates` pose candidates for each example in batch\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared part\n",
    "        self.ensemble = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.predictors = nn.ModuleList([pose_predictor(hidden_dim) for i in range(num_candidates)])\n",
    "\n",
    "        self.student = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            pose_predictor(hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"Transform hidden vector to rotation quaternions\"\n",
    "        student_quat = self.student(z)\n",
    "\n",
    "        if not self.training:\n",
    "            return student_quat\n",
    "        \n",
    "        ensemble = self.ensemble(z)\n",
    "        all_quats = [p(ensemble) for p in self.predictors]\n",
    "        ensemble_quat = torch.cat(all_quats, dim=-1).view(-1, 4)\n",
    "\n",
    "        return torch.cat([ensemble_quat, student_quat], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SupervisedModel\n",
    "\n",
    "`SupervisedModel` uses camera supervision and only predicts and projects point clouds to different cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:35:05.833446Z",
     "start_time": "2019-12-18T12:35:05.814854Z"
    }
   },
   "outputs": [],
   "source": [
    "class SupervisedModel(nn.Module):\n",
    "    \"Basic model\"\n",
    "    def __init__(\n",
    "        self, img_size=128, hidden_dim=1024, num_points=8000, \n",
    "        vox_size=64, smooth_sigma=3.0, predict_scale=True, keep_prob=0.07,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(img_size, hidden_dim)\n",
    "        self.decoder = PointCloudDecoder(num_points, hidden_dim, predict_scale)\n",
    "        self.pc_dropout = PointCloudDropout(keep_prob)\n",
    "        self.pc_projection = Projection(vox_size, smooth_sigma=smooth_sigma)\n",
    "\n",
    "        self.encoder.apply(weight_init)\n",
    "        self.decoder.apply(weight_init)\n",
    "\n",
    "    def forward(self, imgs, poses):\n",
    "        \"Generate new view of `imgs` from `cameras` using differentiable projection\"\n",
    "        bs = imgs.size(0)\n",
    "        num_views = poses.size(0) // bs\n",
    "\n",
    "        z = self.encoder(imgs)\n",
    "        pc, scale = self.decoder(z)\n",
    "\n",
    "        pc    = repeat_tensor_batch(self.pc_dropout(pc), num_views)\n",
    "        scale = repeat_tensor_batch(scale, num_views)\n",
    "        proj  = self.pc_projection(pc, poses, scale)\n",
    "\n",
    "        return proj\n",
    "    \n",
    "\n",
    "class SupervisedLoss(nn.Module):\n",
    "    \"Mse loss / 2\"\n",
    "    def forward(self, proj, masks, **kwargs):\n",
    "        masks = F.interpolate(masks.unsqueeze(0), scale_factor=1/2, mode='bilinear', align_corners=True).squeeze()\n",
    "        return dict(full_loss=F.mse_loss(proj, masks, reduction='sum') / (2 * proj.size(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:38:45.562322Z",
     "start_time": "2019-12-18T12:38:45.537400Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnsupervisedModel(nn.Module):\n",
    "    \"Unsupervised model with ensemble of pose predictors\"\n",
    "    def __init__(self, img_size=128, vox_size=64, \n",
    "                 z_dim=1024, pose_dim=128, \n",
    "                 num_points=8000, num_candidates=4, num_views=5):\n",
    "        super().__init__()\n",
    "        self.num_views = num_views\n",
    "        self.num_candidates = num_candidates\n",
    "\n",
    "        self.encoder = Encoder(img_size, z_dim)\n",
    "        self.pc_decoder = PointCloudDecoder(num_points, hidden_dim=z_dim)\n",
    "        self.pc_dropout = PointCloudDropout()\n",
    "        self.pc_projection = Projection(vox_size)\n",
    "        self.pose_decoder = PoseDecoder(input_dim=z_dim, hidden_dim=pose_dim, num_candidates=num_candidates)\n",
    "        \n",
    "        self.encoder.apply(weight_init)\n",
    "        self.pc_decoder.apply(weight_init)\n",
    "        self.pose_decoder.apply(weight_init)\n",
    "\n",
    "    def forward(self, imgs, poses):\n",
    "        z, z_p = self.encoder(imgs), self.encoder(poses)\n",
    "        pc, scale = self.pc_decoder(z)\n",
    "        poses = self.pose_decoder(z_p)\n",
    "\n",
    "        # No ensemble on inference\n",
    "        if not self.training:\n",
    "            bs = imgs.size(0)\n",
    "            pc    = repeat_tensor_batch(self.pc_dropout(pc), self.num_views)\n",
    "            scale = repeat_tensor_batch(scale, self.num_views)\n",
    "            proj  = self.pc_projection(pc, poses, scale)\n",
    "            return proj, poses\n",
    "\n",
    "        bs = imgs.size(0) * self.num_views\n",
    "        ensemble_poses, student_poses = poses[:-bs], poses[-bs:]\n",
    "\n",
    "        pc    = repeat_tensor_batch(self.pc_dropout(pc), self.num_candidates * self.num_views)\n",
    "        scale = repeat_tensor_batch(scale, self.num_candidates * self.num_views)\n",
    "        proj = self.pc_projection(pc, ensemble_poses, scale)\n",
    "\n",
    "        return proj, ensemble_poses, student_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T13:06:05.791063Z",
     "start_time": "2019-12-18T13:06:05.770579Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnsupervisedLoss(nn.Module):\n",
    "    \"Loss combines projection losses for ensemble and student loss\"\n",
    "    def __init__(self, num_candidates=4, student_weight=20.0):\n",
    "        super().__init__()\n",
    "        self.student_weight = student_weight\n",
    "        self.num_candidates = num_candidates\n",
    "    \n",
    "    def forward(self, pred, masks, training):\n",
    "        proj, *poses = pred\n",
    "        masks = F.interpolate(masks.unsqueeze(0), scale_factor=1/2, mode='bilinear', align_corners=True).squeeze()\n",
    "\n",
    "        if not training:\n",
    "            return dict(projection_loss=F.mse_loss(proj, masks, reduction='sum') / proj.size(0))\n",
    "        \n",
    "        ensemble_poses, student_poses = poses\n",
    "        masks = masks.repeat(self.num_candidates, 1, 1)\n",
    "\n",
    "        projection_loss = F.mse_loss(proj, masks, reduction='none')\n",
    "        projection_loss = projection_loss.sum((1,2)).view(-1, self.num_candidates)\n",
    "        min_idxs = projection_loss.argmin(dim=-1).detach()\n",
    "        batch_idxs = torch.arange(min_idxs.size(0), device=min_idxs.device)\n",
    "\n",
    "        # Student loss\n",
    "        min_projection_loss = projection_loss[batch_idxs, min_idxs].sum() / min_idxs.size(0)\n",
    "        ensemble_poses = ensemble_poses.view(-1, self.num_candidates, 4)\n",
    "        best_poses = ensemble_poses[batch_idxs, min_idxs, :].detach()\n",
    "        \n",
    "        poses_diff = F.normalize(quatmul(best_poses, quatconj(student_poses)), dim=-1)\n",
    "        angle_diff = poses_diff[:, 0]\n",
    "        student_loss = (1 - angle_diff**2).sum() / min_idxs.size(0)\n",
    "\n",
    "        # Save to print histogram\n",
    "        self.min_idxs = min_idxs.detach()\n",
    "\n",
    "        return dict(\n",
    "            projection_loss=min_projection_loss,\n",
    "            student_loss=student_loss,\n",
    "            full_loss=min_projection_loss + self.student_weight * student_loss,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use splits and renders from authors of the paper (but we train on train + test and validate on val). There is an option in dataset to load camera positions instead of other views. We always load all 5 renders (for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T09:19:58.757949Z",
     "start_time": "2019-12-18T09:18:01.838906Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget --quiet --show-progress \"https://datasets.d2.mpi-inf.mpg.de/unsupervised-shape-pose/{DataBunch._ids[\"chairs\"]}-renders.tar.gz\"\n",
    "!wget --quiet --show-progress \"https://datasets.d2.mpi-inf.mpg.de/unsupervised-shape-pose/{DataBunch._ids[\"planes\"]}-renders.tar.gz\"\n",
    "!wget --quiet --show-progress \"https://datasets.d2.mpi-inf.mpg.de/unsupervised-shape-pose/{DataBunch._ids[\"cars\"]}-renders.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T09:22:25.108353Z",
     "start_time": "2019-12-18T09:21:41.751401Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar -xzf \"{DataBunch._ids[\"chairs\"]}-renders.tar.gz\"\n",
    "!tar -xzf \"{DataBunch._ids[\"planes\"]}-renders.tar.gz\"\n",
    "!tar -xzf \"{DataBunch._ids[\"cars\"]}-renders.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T09:24:38.680466Z",
     "start_time": "2019-12-18T09:24:38.357604Z"
    }
   },
   "outputs": [],
   "source": [
    "!mv \"{DataBunch._ids[\"chairs\"]}\" data\n",
    "!mv \"{DataBunch._ids[\"planes\"]}\" data\n",
    "!mv \"{DataBunch._ids[\"cars\"]}\" data\n",
    "\n",
    "!rm \"{DataBunch._ids[\"chairs\"]}-renders.tar.gz\"\n",
    "!rm \"{DataBunch._ids[\"planes\"]}-renders.tar.gz\"\n",
    "!rm \"{DataBunch._ids[\"cars\"]}-renders.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T18:57:56.224633Z",
     "start_time": "2019-12-19T18:57:56.192634Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_models(path=\".\", shapenet_id=\"03001627\", split=\"train\"):\n",
    "    \"Read model paths from split file\"\n",
    "    path = Path(path)\n",
    "\n",
    "    assert split in (\"train\", \"valid\")\n",
    "    split = path/f\"{shapenet_id}.{split}\"\n",
    "    data = path/shapenet_id\n",
    "\n",
    "    with open(split) as models:\n",
    "        return [data/m.strip() for m in models]\n",
    "\n",
    "\n",
    "class Shapenet(Dataset):\n",
    "    \"Dataset with renders and views for shapenet category\"\n",
    "    def __init__(self, models, camera=True, img_size=128):\n",
    "        self.models = models\n",
    "        self.camera = camera\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        model = self.models[idx]\n",
    "        images = []\n",
    "        masks = []\n",
    "        cameras = []\n",
    "\n",
    "        for name in sorted(os.listdir(model)):\n",
    "            if name.startswith(\"render\"):\n",
    "                o = np.array(T.resize(Image.open(model/name), (self.img_size, self.img_size)))\n",
    "                mask = o[..., -1].astype(np.float32) / 255.\n",
    "                img = o[..., :-1].astype(np.float32) / 255.\n",
    "                \n",
    "                images.append(torch.tensor(img).permute(2, 0, 1))\n",
    "                masks.append(torch.tensor(mask))\n",
    "                \n",
    "            if name.startswith(\"camera\"):\n",
    "                camera = loadmat(model/name)\n",
    "                cameras.append(quat_from_campos(camera[\"pos\"]))\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        masks = torch.stack(masks)\n",
    "        if self.camera: poses = torch.stack(cameras)\n",
    "        else:           poses = images\n",
    "\n",
    "        return images, poses, masks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate function handles sampling one image for point cloud generation. `views` might be cameras or images (depends on dataset configuration, but either way it is some information about other views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:46:07.020457Z",
     "start_time": "2019-12-18T11:46:07.011539Z"
    }
   },
   "outputs": [],
   "source": [
    "def multi_view_collate(batch):\n",
    "    \"Prepare batch with 1 image and n_poses masks and poses per item\"\n",
    "    bs = len(batch)\n",
    "    n_poses = batch[0][0].size(0)\n",
    "\n",
    "    idxs = torch.randint(0, n_poses, size=(bs,))\n",
    "    imgs, poses, masks = zip(*[(img[i], view, mask) for (img, view, mask), i in zip(batch, idxs)])\n",
    "\n",
    "    imgs = torch.stack(imgs)\n",
    "    poses = torch.cat(poses, dim=0)\n",
    "    masks = torch.cat(masks, dim=0)\n",
    "    \n",
    "    return imgs, poses, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets and dataloaders all in one class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T18:58:32.966330Z",
     "start_time": "2019-12-19T18:58:32.951410Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    _ids = {\n",
    "        \"chairs\": \"03001627\",\n",
    "        \"planes\": \"02691156\",\n",
    "        \"cars\": \"02958343\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, path, category=\"chairs\", batch_size=10, img_size=128, camera=True):\n",
    "        train = get_models(path, self._ids[category], \"train\")\n",
    "        valid = get_models(path, self._ids[category], \"valid\")\n",
    "        self.train_ds, self.valid_ds = Shapenet(train, camera, img_size), Shapenet(valid, camera, img_size)\n",
    "        self.train_dl = DataLoader(\n",
    "            self.train_ds, batch_size, \n",
    "            shuffle=True, collate_fn=multi_view_collate, drop_last=True, \n",
    "            pin_memory=torch.cuda.is_available(), num_workers=4,\n",
    "        )\n",
    "        self.valid_dl = DataLoader(\n",
    "            self.valid_ds, batch_size * 2, \n",
    "            shuffle=False, collate_fn=multi_view_collate, \n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to overfit one example and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Overfit one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = DataBunch(path=\"data\", batch_size=10, camera=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(10)\n",
    "model = UnsupervisedModel().cuda().train()\n",
    "loss = UnsupervisedLoss()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "torch.random.manual_seed(10)\n",
    "imgs, poses, masks = next(iter(data.train_dl))\n",
    "\n",
    "imgs = imgs.cuda()\n",
    "poses = poses.cuda()\n",
    "masks = masks.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T12:40:17.492547Z",
     "start_time": "2019-12-04T12:39:40.981177Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf6cd84f0104f4bbb8e1d66aa05e0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(range(1000))\n",
    "\n",
    "for i in pbar:\n",
    "    t1 = time.perf_counter()\n",
    "    proj = model(imgs, poses)\n",
    "\n",
    "    l = loss(proj, masks, True)\n",
    "    l[\"full_loss\"].backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    dt = time.perf_counter() - t1\n",
    "\n",
    "    pbar.set_postfix(time=dt, loss=l[\"full_loss\"].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:03:46.565172Z",
     "start_time": "2019-12-18T12:03:46.560960Z"
    }
   },
   "outputs": [],
   "source": [
    "def loopy(dl):\n",
    "    \"Loop through dataloader indefinitley\"\n",
    "    while True:\n",
    "        for o in dl: yield o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:03:45.526347Z",
     "start_time": "2019-12-18T12:03:45.517285Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_params(model, step, keep_prob=(0.07, 1.0), sigma=(3.0, 0.2)):\n",
    "    \"Schedule model parameters linearly (dropout keep_prob and smoothing sigma)\"\n",
    "    assert 0 <= step <= 1\n",
    "\n",
    "    new_keep_prob = keep_prob[0] * (1 - step) + keep_prob[1] * step\n",
    "    new_sigma = sigma[0] * (1 - step) + sigma[1] * step\n",
    "    \n",
    "    model.pc_dropout.keep_prob = new_keep_prob\n",
    "    model.pc_projection.sigma  = torch.empty_like(model.pc_projection.sigma).fill_(new_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for training models and capturing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T13:06:22.628138Z",
     "start_time": "2019-12-18T13:06:22.594893Z"
    }
   },
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    \"Class for training a model\"\n",
    "    def __init__(self, path, data, model, loss, lr=1e-4, wd=0.001, seed=100):\n",
    "        torch.random.manual_seed(seed)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.path = Path(path)\n",
    "        if self.path.exists():\n",
    "            shutil.rmtree(self.path)\n",
    "        (self.path/\"models\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        self.train_writer = SummaryWriter(log_dir=self.path/\"logs\"/\"train\")\n",
    "        self.valid_writer = SummaryWriter(log_dir=self.path/\"logs\"/\"valid\")\n",
    "        self.valid_losses = []\n",
    "\n",
    "        self.data = data\n",
    "        self.model = model.to(self.device)\n",
    "        self.loss = loss\n",
    "        self.opt = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    def fit(self, steps=300_000, eval_every=10_000, vis_every=1000):\n",
    "        \"Train model for `steps` steps\"\n",
    "        self.pbar = tqdm(range(1, steps + 1), desc=\"Step\")\n",
    "        train_dl = loopy(self.data.train_dl)\n",
    "\n",
    "        for step in self.pbar:\n",
    "            self.model.train()\n",
    "            adjust_params(self.model, step / steps)\n",
    "            self.step = step\n",
    "            self.imgs, self.poses, self.masks = next(train_dl)\n",
    "            self.one_batch()\n",
    "\n",
    "            if step % eval_every == 0:\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for self.imgs, self.poses, self.masks in tqdm(self.data.valid_dl, leave=False):\n",
    "                        self.one_batch()\n",
    "                    self.write_valid_losses()\n",
    "\n",
    "                torch.save(\n",
    "                    dict(model=self.model.state_dict(), opt=self.opt.state_dict(), step=self.step), \n",
    "                    self.path/\"models\"/f\"model_{self.step}.pth\"\n",
    "                )\n",
    "\n",
    "            if step % vis_every == 0:\n",
    "                self.model.eval()\n",
    "                imgs, poses, masks = self.data.valid_ds[10]\n",
    "                renders = generate_projections_img(self.model, imgs, poses, masks)\n",
    "                self.train_writer.add_images(\"renders\", renders, self.step)\n",
    "\n",
    "\n",
    "    def one_batch(self):\n",
    "        \"Run one batch of a model\"\n",
    "        device = self.device\n",
    "        imgs, poses, masks = self.imgs.to(device),self.poses.to(device),self.masks.to(device)\n",
    "        proj = self.model(imgs, poses)\n",
    "        t1 = time.perf_counter()\n",
    "        loss = self.loss(proj, masks, training=self.model.training)\n",
    "        if not self.model.training:\n",
    "            self.valid_losses.append({key: l.item() for key, l in loss.items()})\n",
    "            return\n",
    "\n",
    "        loss['full_loss'].backward()\n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()\n",
    "        dt = time.perf_counter() - t1\n",
    "        self.pbar.set_postfix(time=dt, loss=loss['full_loss'].item())\n",
    "\n",
    "        min_idxs = getattr(self.loss, \"min_idxs\", None)\n",
    "        if min_idxs is not None:\n",
    "            self.train_writer.add_histogram(\"other/predictors\", min_idxs.cpu(), self.step)\n",
    "\n",
    "        for key, l in loss.items():\n",
    "            self.train_writer.add_scalar(key, l.item(), self.step)\n",
    "\n",
    "    def write_valid_losses(self):\n",
    "        \"Calculate means for all validation losses and write to tensorboard\"\n",
    "        means = defaultdict(int)\n",
    "        for loss in self.valid_losses:\n",
    "            for key, val in loss.items():\n",
    "                means[key] += val\n",
    "\n",
    "        for key in means.keys():\n",
    "            self.valid_writer.add_scalar(key, means[key] / len(self.valid_losses), self.step)\n",
    "            print(f\"{key}={means[key]/len(self.valid_losses):.3f}\", end=\" \")\n",
    "        print()\n",
    "        self.valid_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T19:08:27.866555Z",
     "start_time": "2019-12-19T19:08:27.710538Z"
    }
   },
   "outputs": [],
   "source": [
    "data = DataBunch(path=\"data\", img_size=64, batch_size=12, camera=False)\n",
    "learner = Learner(\n",
    "    \"./chairs_unsupervised\", \n",
    "    data, \n",
    "    UnsupervisedModel(img_size=64, vox_size=32, num_points=2000), \n",
    "    UnsupervisedLoss(), \n",
    "    lr=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T19:08:55.156002Z",
     "start_time": "2019-12-19T19:08:28.795419Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.fit(steps=600_000, eval_every=20_000, vis_every=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = DataBunch(path=\"data\", batch_size=2, camera=False)\n",
    "model = UnsupervisedModel()\n",
    "checkpoint = torch.load(\"/Users/irubachev/Downloads/model_80000.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
